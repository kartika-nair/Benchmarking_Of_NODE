{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Visualisation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bofvQt9_v-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from copy import deepcopy\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "print(os.getcwd())\n",
        "if os.getcwd().split(\"/\")[-1] == \"Covid_19_predictor\":\n",
        "  print(\"Current working directory is already Covid_19_predictor\")\n",
        "elif os.path.isdir(\"Covid_19_predictor\"):\n",
        "  print(\"Covid_19_predictor already Exists\")\n",
        "else:\n",
        "  ! git clone https://github.com/saahil-jain/Covid_19_predictor.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dacTIIyevWp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if os.path.isdir(\"Covid_19_predictor\"):\n",
        "  % cd Covid_19_predictor\n",
        "! git pull"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stcxW4Vy_5rr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.read_csv(\"data.csv\")\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eEKp1IGAylh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grouped=df.groupby('countriesAndTerritories')\n",
        "countries=sorted(list(set(df.countriesAndTerritories.unique())))\n",
        "countries\n",
        "country_population = {}\n",
        "for country in countries:\n",
        "  country_population[country]=set(grouped.get_group(country)['popData2018'])\n",
        "countries_to_remove = [\"Cases_on_an_international_conveyance_Japan\", \"Holy_See\"]\n",
        "for country in countries:\n",
        "  if len(country_population[country])>1 :\n",
        "    countries_to_remove.append(country)\n",
        "for country in countries_to_remove:\n",
        "  del country_population[country]\n",
        "  countries.remove(country)\n",
        "\n",
        "country_population = {}\n",
        "for country in countries:\n",
        "  country_population[country]=list(grouped.get_group(country)['popData2018'])[0]\n",
        "#for country in countries:\n",
        "#    print(\"{0:45s}\".format(country), country_population[country])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJRm3E37Cgym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "country_wise_cases=dict()\n",
        "country_wise_deaths=dict()\n",
        "normalised_country_wise_cases=dict()\n",
        "normalised_country_wise_deaths=dict()\n",
        "for country in countries:\n",
        "  country_wise_cases[country]=list(grouped.get_group(country)['cases'])[::-1]\n",
        "  country_wise_deaths[country]=list(grouped.get_group(country)['deaths'])[::-1]\n",
        "for country in countries:\n",
        "  for index in range(len(country_wise_cases[country])):\n",
        "    country_wise_cases[country][index] = abs(country_wise_cases[country][index])\n",
        "    country_wise_deaths[country][index] = abs(country_wise_deaths[country][index])\n",
        "\n",
        "for country in countries:\n",
        "  leading_zero_count = 0\n",
        "  for index in range(len(country_wise_cases[country])):\n",
        "    if country_wise_cases[country][index] == 0:\n",
        "      leading_zero_count += 1\n",
        "    else:\n",
        "      country_wise_cases[country] = country_wise_cases[country][leading_zero_count:]\n",
        "      country_wise_deaths[country] = country_wise_deaths[country][leading_zero_count:]\n",
        "      break\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o7N4Et9DLzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for country in countries:\n",
        "  graph=plt.plot(country_wise_cases[country])\n",
        "  # plt.legend(countries,loc='lower left')\n",
        "plt.show()\n",
        "for country in countries:\n",
        "  normalised_country_wise_cases[country] = deepcopy(country_wise_cases[country])\n",
        "  for index in range(len(country_wise_cases[country])):\n",
        "    normalised_country_wise_cases[country][index] = normalised_country_wise_cases[country][index] * 1_000_000 / country_population[country]\n",
        "  graph=plt.plot(normalised_country_wise_cases[country])\n",
        "  # plt.legend(countries,loc='lower left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyLSSV-jKFv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for country in countries:\n",
        "  graph=plt.plot(country_wise_deaths[country])\n",
        "  # plt.legend(countries,loc='lower left')\n",
        "plt.show()\n",
        "\n",
        "for country in countries:\n",
        "  normalised_country_wise_deaths[country] = deepcopy(country_wise_deaths[country])\n",
        "  for index in range(len((country_wise_deaths[country]))):\n",
        "    normalised_country_wise_deaths[country][index] = normalised_country_wise_deaths[country][index] * 1_000_000 / country_population[country]\n",
        "  graph=plt.plot(normalised_country_wise_deaths[country])\n",
        "  # plt.legend(countries,loc='lower left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8knzf4KKL_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cummulative_country_wise_cases=dict()\n",
        "cummulative_country_wise_deaths=dict()\n",
        "normalised_cummulative_country_wise_cases=dict()\n",
        "normalised_cummulative_country_wise_deaths=dict()\n",
        "\n",
        "for country in countries:\n",
        "  cummulative_country_wise_cases[country] = []\n",
        "  cummulative_country_wise_deaths[country] = []\n",
        "  cummulative_cases = 0\n",
        "  cummulative_death = 0\n",
        "  normalised_cummulative_country_wise_cases[country] = []\n",
        "  normalised_cummulative_country_wise_deaths[country] = []\n",
        "  normalised_cummulative_cases = 0\n",
        "  normalised_cummulative_death = 0\n",
        "  total_days = len(country_wise_cases[country])\n",
        "  for index in range(total_days):\n",
        "    cummulative_cases += country_wise_cases[country][index]\n",
        "    cummulative_death += country_wise_deaths[country][index]\n",
        "    cummulative_country_wise_cases[country].append(cummulative_cases)\n",
        "    cummulative_country_wise_deaths[country].append(cummulative_death)\n",
        "    normalised_cummulative_cases += normalised_country_wise_cases[country][index]\n",
        "    normalised_cummulative_death += normalised_country_wise_deaths[country][index]\n",
        "    normalised_cummulative_country_wise_cases[country].append(normalised_cummulative_cases)\n",
        "    normalised_cummulative_country_wise_deaths[country].append(normalised_cummulative_death)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YIeKhBOOXPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for country in countries:\n",
        "  graph=plt.plot(cummulative_country_wise_cases[country])\n",
        "  # plt.legend(countries,loc='lower left')\n",
        "plt.show()\n",
        "for country in countries:\n",
        "  graph=plt.plot(normalised_cummulative_country_wise_cases[country])\n",
        "  # plt.legend(countries,loc='lower left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9uL6CT6bC44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for country in countries:\n",
        "  graph=plt.plot(cummulative_country_wise_deaths[country])\n",
        "  # plt.legend(countries,loc='lower left')\n",
        "plt.show()\n",
        "for country in countries:\n",
        "  graph=plt.plot(normalised_cummulative_country_wise_deaths[country])\n",
        "  # plt.legend(countries,loc='lower left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZWGnVFgbHRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for country in countries:\n",
        "  print(\"{0:45s}\".format(country),\"{0:10d}\".format(cummulative_country_wise_cases[country][-1]),\"{0:10d}\".format(cummulative_country_wise_deaths[country][-1]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mVV_oV8BoQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#code to generate a list of top 15 countries with highest cummulative deaths and cases.\n",
        "#generates two lists of countries which are sorted in descending order based on the cases/deaths\n",
        "#adjust value of slicing to get top 10/20.\n",
        "top_n = 10\n",
        "countries_and_cases=[]\n",
        "countries_and_deaths=[]\n",
        "\n",
        "for country,cumm_cases in zip(cummulative_country_wise_cases.keys(),cummulative_country_wise_cases.values()):\n",
        "  countries_and_cases.append([country,cumm_cases[-1]])\n",
        "countries_and_cases=sorted(countries_and_cases,key=lambda x:x[1],reverse=True)\n",
        "top_n_cases=list(map(lambda x:x[0],countries_and_cases))[:top_n]\n",
        "\n",
        "\n",
        "for country,cumm_deaths in zip(cummulative_country_wise_deaths.keys(),cummulative_country_wise_deaths.values()):\n",
        "  countries_and_deaths.append([country,cumm_deaths[-1]])\n",
        "countries_and_deaths=sorted(countries_and_deaths,key=lambda x:x[1],reverse=True)\n",
        "\n",
        "top_n_deaths=list(map(lambda x:x[0],countries_and_deaths))[:top_n]\n",
        "\n",
        "print(\"{0:45s}\".format(\"Total Cases\"), \"{0:45s}\".format(\"Total Deaths\"), end=\"\\n\\n\")\n",
        "for index in range(top_n):\n",
        "  print(\"{0:45s}\".format(top_n_cases[index]), \"{0:45s}\".format(top_n_deaths[index]))\n",
        "\n",
        "\n",
        "def intersection(lst1, lst2): \n",
        "    return list(set(lst1) & set(lst2)) \n",
        "training_countries = list(intersection(top_n_cases, top_n_deaths))\n",
        "training_countries.append(\"India\")\n",
        "training_countries.remove(\"Brazil\")\n",
        "print(training_countries) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izAHpcjRlfUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.style.use('seaborn-whitegrid')\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional\n",
        "from keras.optimizers import SGD\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def return_rmse(test,predicted):\n",
        "    rmse = math.sqrt(mean_squared_error(test, predicted))\n",
        "    print(\"The root mean squared error is {}.\".format(rmse))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUsTLEuhpkj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_data_cases = normalised_country_wise_cases\n",
        "base_data_deaths = normalised_country_wise_deaths\n",
        "\n",
        "def split(dataframe, split_factor):\n",
        "  base_data = {}\n",
        "  for country in training_countries:\n",
        "    base_data[country] = {}\n",
        "    country_data_length = len(dataframe[country])\n",
        "    split_index = int(math.floor(country_data_length * split_factor))\n",
        "    base_data[country][\"Train\"] = dataframe[country][:split_index]\n",
        "    base_data[country][\"Test\"] = dataframe[country][split_index:]\n",
        "  return base_data\n",
        "\n",
        "split_factor = 0.8\n",
        "\n",
        "transform_train_cases = {}\n",
        "transform_test_cases = {}\n",
        "scaler_cases = {}\n",
        "data_cases = split(base_data_cases, split_factor)\n",
        "for num, country in enumerate(training_countries):\n",
        "    sc = MinMaxScaler(feature_range=(0,1))\n",
        "    a0 = np.array(data_cases[country][\"Train\"])\n",
        "    a1 = np.array(data_cases[country][\"Test\"])\n",
        "    a0 = a0.reshape(a0.shape[0],1)\n",
        "    a1 = a1.reshape(a1.shape[0],1)\n",
        "    transform_train_cases[country] = sc.fit_transform(a0)\n",
        "    transform_test_cases[country] = sc.fit_transform(a1)\n",
        "    scaler_cases[country] = sc\n",
        "del a0\n",
        "del a1\n",
        "\n",
        "transform_train_deaths = {}\n",
        "transform_test_deaths = {}\n",
        "scaler_deaths = {}\n",
        "data_deaths = split(base_data_deaths, split_factor)\n",
        "for num, country in enumerate(training_countries):\n",
        "    sc = MinMaxScaler(feature_range=(0,1))\n",
        "    a0 = np.array(data_deaths[country][\"Train\"])\n",
        "    a1 = np.array(data_deaths[country][\"Test\"])\n",
        "    a0 = a0.reshape(a0.shape[0],1)\n",
        "    a1 = a1.reshape(a1.shape[0],1)\n",
        "    transform_train_deaths[country] = sc.fit_transform(a0)\n",
        "    transform_test_deaths[country] = sc.fit_transform(a1)\n",
        "    scaler_deaths[country] = sc\n",
        "del a0\n",
        "del a1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb9-tNSV6tWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in transform_train_cases.keys():\n",
        "    print(i, transform_train_cases[i].shape)\n",
        "print(\"\\n\")    \n",
        "for i in transform_test_cases.keys():\n",
        "    print(i, transform_test_cases[i].shape)\n",
        " \n",
        "print(\"\\n\")    \n",
        "for i in transform_train_deaths.keys():\n",
        "    print(i, transform_train_deaths[i].shape)\n",
        "print(\"\\n\")    \n",
        "for i in transform_test_deaths.keys():\n",
        "    print(i, transform_test_deaths[i].shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w5D5EwO7ObL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainset_cases = {}\n",
        "testset_cases = {}\n",
        "look_back_cases = 3\n",
        "for country in training_countries:\n",
        "    trainset_cases[country] = {}\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    for i in range(look_back_cases,100):\n",
        "        X_train.append(transform_train_cases[country][i-look_back_cases:i,0])\n",
        "        y_train.append(transform_train_cases[country][i,0])\n",
        "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "    trainset_cases[country][\"X\"] = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\n",
        "    trainset_cases[country][\"y\"] = y_train\n",
        "    \n",
        "    testset_cases[country] = {}\n",
        "    X_test = []\n",
        "    y_test = []    \n",
        "    for i in range(look_back_cases, 25):\n",
        "        X_test.append(transform_test_cases[country][i-look_back_cases:i,0])\n",
        "        y_test.append(transform_test_cases[country][i,0])\n",
        "    X_test, y_test = np.array(X_test), np.array(y_test)\n",
        "    testset_cases[country][\"X\"] = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
        "    testset_cases[country][\"y\"] = y_test\n",
        "\n",
        "trainset_deaths = {}\n",
        "testset_deaths = {}\n",
        "look_back_deaths = 3\n",
        "for country in training_countries:\n",
        "    trainset_deaths[country] = {}\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    for i in range(look_back_deaths,100):\n",
        "        X_train.append(transform_train_deaths[country][i-look_back_deaths:i,0])\n",
        "        y_train.append(transform_train_deaths[country][i,0])\n",
        "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "    trainset_deaths[country][\"X\"] = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\n",
        "    trainset_deaths[country][\"y\"] = y_train\n",
        "    \n",
        "    testset_deaths[country] = {}\n",
        "    X_test = []\n",
        "    y_test = []    \n",
        "    for i in range(look_back_deaths, 25):\n",
        "        X_test.append(transform_test_deaths[country][i-look_back_deaths:i,0])\n",
        "        y_test.append(transform_test_deaths[country][i,0])\n",
        "    X_test, y_test = np.array(X_test), np.array(y_test)\n",
        "    testset_deaths[country][\"X\"] = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))\n",
        "    testset_deaths[country][\"y\"] = y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCoSahne9eA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "arr_buff_cases = []\n",
        "for country in training_countries:\n",
        "    buff = {}\n",
        "    buff[\"X_train\"] = trainset_cases[country][\"X\"].shape\n",
        "    buff[\"y_train\"] = trainset_cases[country][\"y\"].shape\n",
        "    buff[\"X_test\"] = testset_cases[country][\"X\"].shape\n",
        "    buff[\"y_test\"] = testset_cases[country][\"y\"].shape\n",
        "    arr_buff_cases.append(buff)\n",
        "\n",
        "pd.DataFrame(arr_buff_cases, index=training_countries)\n",
        "\n",
        "arr_buff_deaths = []\n",
        "for country in training_countries:\n",
        "    buff = {}\n",
        "    buff[\"X_train\"] = trainset_deaths[country][\"X\"].shape\n",
        "    buff[\"y_train\"] = trainset_deaths[country][\"y\"].shape\n",
        "    buff[\"X_test\"] = testset_deaths[country][\"X\"].shape\n",
        "    buff[\"y_test\"] = testset_deaths[country][\"y\"].shape\n",
        "    arr_buff_deaths.append(buff)\n",
        "\n",
        "pd.DataFrame(arr_buff_deaths, index=training_countries)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suSdQhjCfB65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_error = dict()\n",
        "model_types = [\"RNN\",\"LSTM\", \"GRU\", \"NODE\"]\n",
        "for model_type in model_types:\n",
        "  total_error[model_type] = dict()\n",
        "  total_error[model_type][\"Cases\"] = 0\n",
        "  total_error[model_type][\"Deaths\"] = 0\n",
        "  \n",
        "def lagging(df, lag):\n",
        "    df_pred = pd.Series(df[\"Pred\"].reshape(-1))\n",
        "    df_true = pd.Series(df[\"True\"].reshape(-1))\n",
        "    \n",
        "    df_pred_lag = df_pred.shift(lag)\n",
        "    \n",
        "    # print(\"MSE without Lag :\", mean_squared_error(np.array(df_true), np.array(df_pred)))\n",
        "    MSE_lag = mean_squared_error(np.array(df_true[:-lag]), np.array(df_pred_lag[:-lag]))\n",
        "    print(\"MSE with Lag    :\", MSE_lag)\n",
        "\n",
        "    # plt.figure(figsize=(14,4))\n",
        "    # plt.title(\"Prediction without Lag\")\n",
        "    # plt.plot(df_true, color='green')\n",
        "    # plt.plot(df_pred, color='blue')\n",
        "    # plt.legend([\"True\", \"Predicted\"])\n",
        "\n",
        "    plt.figure(figsize=(14,4))\n",
        "    plt.title(\"Prediction with Lag\")\n",
        "    plt.plot(df_true, color='green')\n",
        "    plt.plot(df_pred_lag, color='blue')\n",
        "    plt.legend([\"True\", \"Predicted\"])\n",
        "\n",
        "    return MSE_lag\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGez0OpnYmbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "#\n",
        "%%time\n",
        "epochs = 10\n",
        "# The LSTM architecture\n",
        "regressorRNN_cases = Sequential()\n",
        "# First LSTM layer with Dropout regularisation\n",
        "regressorRNN_cases.add(LSTM(units=50, return_sequences=True, input_shape=(look_back_cases,1)))\n",
        "regressorRNN_cases.add(Dropout(0.2))\n",
        "# Second LSTM layer\n",
        "regressorRNN_cases.add(LSTM(units=50, return_sequences=True))\n",
        "regressorRNN_cases.add(Dropout(0.2))\n",
        "# Third LSTM layer\n",
        "regressorRNN_cases.add(LSTM(units=50, return_sequences=True))\n",
        "regressorRNN_cases.add(Dropout(0.5))\n",
        "# Fourth LSTM layer\n",
        "regressorRNN_cases.add(LSTM(units=50))\n",
        "regressorRNN_cases.add(Dropout(0.5))\n",
        "# The output layer\n",
        "regressorRNN_cases.add(Dense(units=1))\n",
        "\n",
        "# Compiling the RNN\n",
        "regressorRNN_cases.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
        "# Fitting to the training set\n",
        "history_cases = {}\n",
        "for country in training_countries:\n",
        "  history_cases[country] = []\n",
        "for epoch in range(epochs):\n",
        "    for country in training_countries:\n",
        "        print(\"Fitting to\", country)\n",
        "        H = regressorRNN_cases.fit(trainset_cases[country][\"X\"], trainset_cases[country][\"y\"], epochs=1, batch_size=16)\n",
        "        history_cases[country].append(H.history[\"loss\"])\n",
        "print(history_cases)\n",
        "\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "for country in training_countries:\n",
        "  plt.plot(np.arange(0, epochs), history_cases[country], label=country)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LHgrY3CYnLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# epochs = 5\n",
        "# The LSTM architecture\n",
        "regressorRNN_deaths = Sequential()\n",
        "# First LSTM layer with Dropout regularisation\n",
        "regressorRNN_deaths.add(LSTM(units=50, return_sequences=True, input_shape=(look_back_deaths,1)))\n",
        "regressorRNN_deaths.add(Dropout(0.2))\n",
        "# Second LSTM layer\n",
        "regressorRNN_deaths.add(LSTM(units=50, return_sequences=True))\n",
        "regressorRNN_deaths.add(Dropout(0.2))\n",
        "# Third LSTM layer\n",
        "regressorRNN_deaths.add(LSTM(units=50, return_sequences=True))\n",
        "regressorRNN_deaths.add(Dropout(0.5))\n",
        "# Fourth LSTM layer\n",
        "regressorRNN_deaths.add(LSTM(units=50))\n",
        "regressorRNN_deaths.add(Dropout(0.5))\n",
        "# The output layer\n",
        "regressorRNN_deaths.add(Dense(units=1))\n",
        "\n",
        "# Compiling the RNN\n",
        "regressorRNN_deaths.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
        "# Fitting to the training set\n",
        "history_deaths = {}\n",
        "for country in training_countries:\n",
        "  history_deaths[country] = []\n",
        "for epoch in range(epochs):\n",
        "    for country in training_countries:\n",
        "        print(\"Fitting to\", country)\n",
        "        H = regressorRNN_deaths.fit(trainset_deaths[country][\"X\"], trainset_deaths[country][\"y\"], epochs=1, batch_size=16)\n",
        "        history_deaths[country].append(H.history[\"loss\"])\n",
        "print(history_deaths)\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "for country in training_countries:\n",
        "  plt.plot(np.arange(0, epochs), history_deaths[country], label=country)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1ayeH17Yykc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_result_cases = {}\n",
        "for country in training_countries:\n",
        "    y_true = scaler_cases[country].inverse_transform(testset_cases[country][\"y\"].reshape(-1,1))\n",
        "    y_pred = scaler_cases[country].inverse_transform(regressorRNN_cases.predict(testset_cases[country][\"X\"]))\n",
        "    MSE = mean_squared_error(y_true, y_pred)\n",
        "    pred_result_cases[country] = {}\n",
        "    pred_result_cases[country][\"True\"] = y_true\n",
        "    pred_result_cases[country][\"Pred\"] = y_pred\n",
        "    \n",
        "    # plt.figure(figsize=(14,6))\n",
        "    # plt.title(\"{} with MSE {:10.4f}\".format(country,MSE))\n",
        "    # plt.plot(y_true, color='green')\n",
        "    # plt.plot(y_pred, color='blue')\n",
        "    # plt.legend([\"True\", \"Predicted\"])\n",
        "\n",
        "pred_result_deaths = {}\n",
        "for country in training_countries:\n",
        "    y_true = scaler_deaths[country].inverse_transform(testset_deaths[country][\"y\"].reshape(-1,1))\n",
        "    y_pred = scaler_deaths[country].inverse_transform(regressorRNN_deaths.predict(testset_deaths[country][\"X\"]))\n",
        "    MSE = mean_squared_error(y_true, y_pred)\n",
        "    pred_result_deaths[country] = {}\n",
        "    pred_result_deaths[country][\"True\"] = y_true\n",
        "    pred_result_deaths[country][\"Pred\"] = y_pred\n",
        "    \n",
        "    # plt.figure(figsize=(14,6))\n",
        "    # plt.title(\"{} with MSE {:10.4f}\".format(country,MSE))\n",
        "    # plt.plot(y_true, color='green')\n",
        "    # plt.plot(y_pred, color='blue')\n",
        "    # plt.legend([\"True\", \"Predicted\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4rKC6SEYnof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_error[\"RNN\"][\"Cases\"] = 0\n",
        "for country in training_countries:\n",
        "  total_error[\"RNN\"][\"Cases\"] += lagging(pred_result_cases[country], -3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TXyWRP3YoFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_error[\"RNN\"][\"Deaths\"] = 0\n",
        "for country in training_countries:\n",
        "  total_error[\"RNN\"][\"Deaths\"] +=lagging(pred_result_deaths[country], -3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0I-fDtw_NVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "epochs = 10\n",
        "# The LSTM architecture\n",
        "regressor_cases = Sequential()\n",
        "# First LSTM layer with Dropout regularisation\n",
        "regressor_cases.add(LSTM(units=50, return_sequences=True, input_shape=(look_back_cases,1)))\n",
        "regressor_cases.add(Dropout(0.2))\n",
        "# Second LSTM layer\n",
        "regressor_cases.add(LSTM(units=50, return_sequences=True))\n",
        "regressor_cases.add(Dropout(0.2))\n",
        "# Third LSTM layer\n",
        "regressor_cases.add(LSTM(units=50, return_sequences=True))\n",
        "regressor_cases.add(Dropout(0.5))\n",
        "# Fourth LSTM layer\n",
        "regressor_cases.add(LSTM(units=50))\n",
        "regressor_cases.add(Dropout(0.5))\n",
        "# The output layer\n",
        "regressor_cases.add(Dense(units=1))\n",
        "\n",
        "# Compiling the RNN\n",
        "regressor_cases.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
        "# Fitting to the training set\n",
        "history_cases = {}\n",
        "for country in training_countries:\n",
        "  history_cases[country] = []\n",
        "for epoch in range(epochs):\n",
        "    for country in training_countries:\n",
        "        print(\"Fitting to\", country)\n",
        "        H = regressor_cases.fit(trainset_cases[country][\"X\"], trainset_cases[country][\"y\"], epochs=1, batch_size=16)\n",
        "        history_cases[country].append(H.history[\"loss\"])\n",
        "print(history_cases)\n",
        "\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "for country in training_countries:\n",
        "  plt.plot(np.arange(0, epochs), history_cases[country], label=country)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq3NGHlHFTZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# epochs = 5\n",
        "# The LSTM architecture\n",
        "regressor_deaths = Sequential()\n",
        "# First LSTM layer with Dropout regularisation\n",
        "regressor_deaths.add(LSTM(units=50, return_sequences=True, input_shape=(look_back_deaths,1)))\n",
        "regressor_deaths.add(Dropout(0.2))\n",
        "# Second LSTM layer\n",
        "regressor_deaths.add(LSTM(units=50, return_sequences=True))\n",
        "regressor_deaths.add(Dropout(0.2))\n",
        "# Third LSTM layer\n",
        "regressor_deaths.add(LSTM(units=50, return_sequences=True))\n",
        "regressor_deaths.add(Dropout(0.5))\n",
        "# Fourth LSTM layer\n",
        "regressor_deaths.add(LSTM(units=50))\n",
        "regressor_deaths.add(Dropout(0.5))\n",
        "# The output layer\n",
        "regressor_deaths.add(Dense(units=1))\n",
        "\n",
        "# Compiling the RNN\n",
        "regressor_deaths.compile(optimizer='rmsprop', loss='mean_squared_error')\n",
        "# Fitting to the training set\n",
        "history_deaths = {}\n",
        "for country in training_countries:\n",
        "  history_deaths[country] = []\n",
        "for epoch in range(epochs):\n",
        "    for country in training_countries:\n",
        "        print(\"Fitting to\", country)\n",
        "        H = regressor_deaths.fit(trainset_deaths[country][\"X\"], trainset_deaths[country][\"y\"], epochs=1, batch_size=16)\n",
        "        history_deaths[country].append(H.history[\"loss\"])\n",
        "print(history_deaths)\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "for country in training_countries:\n",
        "  plt.plot(np.arange(0, epochs), history_deaths[country], label=country)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKLNR_l8HwaQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_result_cases = {}\n",
        "for country in training_countries:\n",
        "    y_true = scaler_cases[country].inverse_transform(testset_cases[country][\"y\"].reshape(-1,1))\n",
        "    y_pred = scaler_cases[country].inverse_transform(regressor_cases.predict(testset_cases[country][\"X\"]))\n",
        "    MSE = mean_squared_error(y_true, y_pred)\n",
        "    pred_result_cases[country] = {}\n",
        "    pred_result_cases[country][\"True\"] = y_true\n",
        "    pred_result_cases[country][\"Pred\"] = y_pred\n",
        "    \n",
        "    # plt.figure(figsize=(14,6))\n",
        "    # plt.title(\"{} with MSE {:10.4f}\".format(country,MSE))\n",
        "    # plt.plot(y_true, color='green')\n",
        "    # plt.plot(y_pred, color='blue')\n",
        "    # plt.legend([\"True\", \"Predicted\"])\n",
        "\n",
        "pred_result_deaths = {}\n",
        "for country in training_countries:\n",
        "    y_true = scaler_deaths[country].inverse_transform(testset_deaths[country][\"y\"].reshape(-1,1))\n",
        "    y_pred = scaler_deaths[country].inverse_transform(regressor_deaths.predict(testset_deaths[country][\"X\"]))\n",
        "    MSE = mean_squared_error(y_true, y_pred)\n",
        "    pred_result_deaths[country] = {}\n",
        "    pred_result_deaths[country][\"True\"] = y_true\n",
        "    pred_result_deaths[country][\"Pred\"] = y_pred\n",
        "    \n",
        "    # plt.figure(figsize=(14,6))\n",
        "    # plt.title(\"{} with MSE {:10.4f}\".format(country,MSE))\n",
        "    # plt.plot(y_true, color='green')\n",
        "    # plt.plot(y_pred, color='blue')\n",
        "    # plt.legend([\"True\", \"Predicted\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwZzoo60M1dP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_error[\"LSTM\"][\"Cases\"] = 0\n",
        "for country in training_countries:\n",
        "  total_error[\"LSTM\"][\"Cases\"] += lagging(pred_result_cases[country], -3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMn8H89zOqvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_error[\"LSTM\"][\"Deaths\"] = 0\n",
        "for country in training_countries:\n",
        "  total_error[\"LSTM\"][\"Deaths\"] +=lagging(pred_result_deaths[country], -3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrVa6sWKQcil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# epochs = 5\n",
        "# The GRU architecture\n",
        "regressorGRU_cases = Sequential()\n",
        "# First GRU layer with Dropout regularisation\n",
        "regressorGRU_cases.add(GRU(units=50, return_sequences=True, input_shape=(look_back_cases,1), activation='tanh'))\n",
        "regressorGRU_cases.add(Dropout(0.2))\n",
        "# Second GRU layer\n",
        "regressorGRU_cases.add(GRU(units=50, return_sequences=True, input_shape=(look_back_cases,1), activation='tanh'))\n",
        "regressorGRU_cases.add(Dropout(0.2))\n",
        "# Third GRU layer\n",
        "regressorGRU_cases.add(GRU(units=50, return_sequences=True, input_shape=(look_back_cases,1), activation='tanh'))\n",
        "regressorGRU_cases.add(Dropout(0.2))\n",
        "# Fourth GRU layer\n",
        "regressorGRU_cases.add(GRU(units=50, activation='tanh'))\n",
        "regressorGRU_cases.add(Dropout(0.2))\n",
        "# The output layer\n",
        "regressorGRU_cases.add(Dense(units=1))\n",
        "# Compiling the RNN\n",
        "regressorGRU_cases.compile(optimizer=SGD(lr=0.01, decay=1e-7, momentum=0.9, nesterov=False),loss='mean_squared_error')\n",
        "# Fitting to the training set\n",
        "history_cases = {}\n",
        "for country in training_countries:\n",
        "  history_cases[country] = []\n",
        "for epoch in range(epochs):\n",
        "    for country in training_countries:\n",
        "        print(\"Fitting to\", country)\n",
        "        H = regressorGRU_cases.fit(trainset_cases[country][\"X\"], trainset_cases[country][\"y\"], epochs=1, batch_size=16)\n",
        "        history_cases[country].append(H.history[\"loss\"])\n",
        "print(history_cases)\n",
        "\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "for country in training_countries:\n",
        "  plt.plot(np.arange(0, epochs), history_cases[country], label=country)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3lZCgOhdBqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "# epochs = 5\n",
        "# The GRU architecture\n",
        "regressorGRU_deaths = Sequential()\n",
        "# First GRU layer with Dropout regularisation\n",
        "regressorGRU_deaths.add(GRU(units=50, return_sequences=True, input_shape=(look_back_deaths,1), activation='tanh'))\n",
        "regressorGRU_deaths.add(Dropout(0.2))\n",
        "# Second GRU layer\n",
        "regressorGRU_deaths.add(GRU(units=50, return_sequences=True, input_shape=(look_back_deaths,1), activation='tanh'))\n",
        "regressorGRU_deaths.add(Dropout(0.2))\n",
        "# Third GRU layer\n",
        "regressorGRU_deaths.add(GRU(units=50, return_sequences=True, input_shape=(look_back_deaths,1), activation='tanh'))\n",
        "regressorGRU_deaths.add(Dropout(0.2))\n",
        "# Fourth GRU layer\n",
        "regressorGRU_deaths.add(GRU(units=50, activation='tanh'))\n",
        "regressorGRU_deaths.add(Dropout(0.2))\n",
        "# The output layer\n",
        "regressorGRU_deaths.add(Dense(units=1))\n",
        "# Compiling the RNN\n",
        "regressorGRU_deaths.compile(optimizer=SGD(lr=0.01, decay=1e-7, momentum=0.9, nesterov=False),loss='mean_squared_error')\n",
        "# Fitting to the training set\n",
        "history_deaths = {}\n",
        "for country in training_countries:\n",
        "  history_deaths[country] = []\n",
        "for epoch in range(epochs):\n",
        "    for country in training_countries:\n",
        "        print(\"Fitting to\", country)\n",
        "        H = regressorGRU_deaths.fit(trainset_deaths[country][\"X\"], trainset_deaths[country][\"y\"], epochs=1, batch_size=16)\n",
        "        history_deaths[country].append(H.history[\"loss\"])\n",
        "print(history_deaths)\n",
        "\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "for country in training_countries:\n",
        "  plt.plot(np.arange(0, epochs), history_deaths[country], label=country)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5uq56gbd44k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_result_cases = {}\n",
        "for country in training_countries:\n",
        "    y_true = scaler_cases[country].inverse_transform(testset_cases[country][\"y\"].reshape(-1,1))\n",
        "    y_pred = scaler_cases[country].inverse_transform(regressorGRU_cases.predict(testset_cases[country][\"X\"]))\n",
        "    MSE = mean_squared_error(y_true, y_pred)\n",
        "    pred_result_cases[country] = {}\n",
        "    pred_result_cases[country][\"True\"] = y_true\n",
        "    pred_result_cases[country][\"Pred\"] = y_pred\n",
        "    \n",
        "    # plt.figure(figsize=(14,6))\n",
        "    # plt.title(\"{} with MSE {:10.4f}\".format(country,MSE))\n",
        "    # plt.plot(y_true, color='green')\n",
        "    # plt.plot(y_pred, color='blue')\n",
        "    # plt.legend([\"True\", \"Predicted\"])\n",
        "\n",
        "pred_result_deaths = {}\n",
        "for country in training_countries:\n",
        "    y_true = scaler_deaths[country].inverse_transform(testset_deaths[country][\"y\"].reshape(-1,1))\n",
        "    y_pred = scaler_deaths[country].inverse_transform(regressorGRU_deaths.predict(testset_deaths[country][\"X\"]))\n",
        "    MSE = mean_squared_error(y_true, y_pred)\n",
        "    pred_result_deaths[country] = {}\n",
        "    pred_result_deaths[country][\"True\"] = y_true\n",
        "    pred_result_deaths[country][\"Pred\"] = y_pred\n",
        "    \n",
        "    # plt.figure(figsize=(14,6))\n",
        "    # plt.title(\"{} with MSE {:10.4f}\".format(country,MSE))\n",
        "    # plt.plot(y_true, color='green')\n",
        "    # plt.plot(y_pred, color='blue')\n",
        "    # plt.legend([\"True\", \"Predicted\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtYbQzBZetZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_error[\"GRU\"][\"Cases\"] = 0\n",
        "for country in training_countries:\n",
        "  total_error[\"GRU\"][\"Cases\"] +=lagging(pred_result_cases[country], -3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrJNWhtNfUJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_error[\"GRU\"][\"Deaths\"] = 0\n",
        "for country in training_countries:\n",
        "  total_error[\"GRU\"][\"Deaths\"] +=lagging(pred_result_deaths[country], -3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqwGiQWGfaLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for model_type in model_types:\n",
        "  print(\"{0:5s}\".format(model_type), total_error[model_type])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TBgn0x3iveS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainset_cases = {}\n",
        "testset_cases = {}\n",
        "look_back_cases = 1\n",
        "for country in training_countries:\n",
        "    trainset_cases[country] = {}\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    for i in range(look_back_cases,100):\n",
        "        X_train.append(transform_train_cases[country][i-look_back_cases:i,0])\n",
        "        y_train.append(transform_train_cases[country][i,0])\n",
        "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "    trainset_cases[country][\"X\"] = X_train\n",
        "    trainset_cases[country][\"y\"] = np.reshape(y_train,(y_train.shape[0],1))\n",
        "    \n",
        "    testset_cases[country] = {}\n",
        "    X_test = []\n",
        "    y_test = []    \n",
        "    for i in range(look_back_cases, 25):\n",
        "        X_test.append(transform_test_cases[country][i-look_back_cases:i,0])\n",
        "        y_test.append(transform_test_cases[country][i,0])\n",
        "    X_test, y_test = np.array(X_test), np.array(y_test)\n",
        "    testset_cases[country][\"X\"] = X_test\n",
        "    testset_cases[country][\"y\"] = np.reshape(y_test,(y_test.shape[0],1))\n",
        "\n",
        "trainset_deaths = {}\n",
        "testset_deaths = {}\n",
        "look_back_deaths = 1\n",
        "for country in training_countries:\n",
        "    trainset_deaths[country] = {}\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    for i in range(look_back_deaths,100):\n",
        "        X_train.append(transform_train_deaths[country][i-look_back_deaths:i,0])\n",
        "        y_train.append(transform_train_deaths[country][i,0])\n",
        "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "    trainset_deaths[country][\"X\"] = X_train\n",
        "    trainset_deaths[country][\"y\"] = np.reshape(y_train,(y_train.shape[0],1))\n",
        "    \n",
        "    testset_deaths[country] = {}\n",
        "    X_test = []\n",
        "    y_test = []    \n",
        "    for i in range(look_back_deaths, 25):\n",
        "        X_test.append(transform_test_deaths[country][i-look_back_deaths:i,0])\n",
        "        y_test.append(transform_test_deaths[country][i,0])\n",
        "    X_test, y_test = np.array(X_test), np.array(y_test)\n",
        "    testset_deaths[country][\"X\"] = X_test\n",
        "    testset_deaths[country][\"y\"] = np.reshape(y_test,(y_test.shape[0],1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KavJTxjuPbOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR, MultiStepLR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA7nEdXsO8-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def norm(dim):\n",
        "    return nn.GroupNorm(min(32, dim), dim)\n",
        "    \n",
        "def zip_map(zipped, update_op):\n",
        "    return [update_op(*elems) for elems in zipped]\n",
        "\n",
        "def euler_update(h_list, dh_list, dt):\n",
        "    return zip_map(zip(h_list, dh_list), lambda h, dh: h + dt * dh)\n",
        "\n",
        "def euler_step(func, dt, state):\n",
        "    return euler_update(state, func(state), dt)\n",
        "\n",
        "def rk2_step(func, dt, state, **kwargs):\n",
        "    k1 = func(state, **kwargs)\n",
        "    k2 = func(euler_update(state, k1, dt), **kwargs)\n",
        "    return zip_map(zip(state, k1, k2),\n",
        "                   lambda h, dk1, dk2: h + dt * (dk1 + dk2) / 2)\n",
        "\n",
        "def rk4_step(func, dt, state, **kwargs):\n",
        "    k1 = func(state, **kwargs)\n",
        "    k2 = func(euler_update(state, k1, dt / 2), **kwargs)\n",
        "    k3 = func(euler_update(state, k2, dt / 2), **kwargs)\n",
        "    k4 = func(euler_update(state, k3, dt), **kwargs)\n",
        "\n",
        "    return zip_map(\n",
        "        zip(state, k1, k2, k3, k4), lambda h, dk1, dk2, dk3, dk4: h + dt * ( dk1 + 2 * dk2 + 2 * dk3 + dk4) / 6,)\n",
        "    \n",
        "def forward_dynamics(state, nnet):\n",
        "    t, y = state\n",
        "    return [1.0, nnet(t, y)]\n",
        "\n",
        "def backward_dynamics(state, nnet):\n",
        "    with torch.set_grad_enabled(True):\n",
        "        t, ht, at = state[0], state[1], state[2]\n",
        "        ht = ht.detach()\n",
        "        ht.requires_grad_(True)\n",
        "        ht_new = nnet(t, ht)\n",
        "        gradients = torch.autograd.grad(ht_new, [ht] + [w for w in nnet.parameters()], at, allow_unused=True, retain_graph=True)\n",
        "    return [1.0, ht_new, *gradients]\n",
        "\n",
        "class NeuralODEFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, nnet, solver, t, *params):\n",
        "        delta_t = t[1:] - t[:-1]\n",
        "\n",
        "        ctx.nnet = nnet\n",
        "        ctx.solver = solver\n",
        "        ctx.delta_t = delta_t\n",
        "\n",
        "        state = [0, input]\n",
        "        for dt in delta_t:\n",
        "            state = solver(func=forward_dynamics, dt=float(dt), state=state, nnet=nnet)\n",
        "        output = state[1]\n",
        "        \n",
        "        ctx.save_for_backward(input, output)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, output_gradients):\n",
        "        input, output = ctx.saved_tensors\n",
        "        nnet = ctx.nnet\n",
        "        solver = ctx.solver\n",
        "        delta_t = ctx.delta_t\n",
        "        params = nnet.parameters()\n",
        "\n",
        "        grad_weights = []\n",
        "        for p in params:\n",
        "            grad_weights.append(torch.zeros_like(p))\n",
        "\n",
        "        state = [1, output, output_gradients, *grad_weights]\n",
        "\n",
        "        for i, dt in enumerate(delta_t):\n",
        "            state = solver(func=backward_dynamics, dt=float(dt), state=state, nnet=nnet)\n",
        "\n",
        "        # input = state[1]\n",
        "        grad_input = state[2]\n",
        "        grad_weights = state[3:]\n",
        "        return (grad_input, None, None, None, *grad_weights)\n",
        "\n",
        "class NeuralODE(nn.Module):\n",
        "    def __init__(self, model, solver=rk4_step, t=np.linspace(0,99,99)):\n",
        "      super().__init__()\n",
        "      self.t = t\n",
        "      self.model = model\n",
        "      self.solver = solver\n",
        "      self.params = [w for w in model.parameters()]\n",
        "\n",
        "    def forward(self, input):\n",
        "      return NeuralODEFunction.apply(input, self.model, self.solver, self.t, *self.params)\n",
        "\n",
        "class ODEfunc(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super(ODEfunc, self).__init__()\n",
        "        self.norm1 = norm(dim)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\n",
        "        self.norm2 = norm(dim)\n",
        "        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\n",
        "        self.norm3 = norm(dim)\n",
        "        self.nfe = 0\n",
        "\n",
        "    def forward(self, t, x):\n",
        "        self.nfe += 1\n",
        "        out = self.norm1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(t, out)\n",
        "        out = self.norm2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(t, out)\n",
        "        out = self.norm3(out)\n",
        "        return out\n",
        "\n",
        "class arguments:\n",
        "  def __init__(self):\n",
        "    self.batch_size=64\n",
        "    self.test_batch_size=1000\n",
        "    self.epochs=15\n",
        "    self.lr=1.0\n",
        "    self.gamma=0.7\n",
        "    self.seed=1\n",
        "    self.log_interval=5\n",
        "    \n",
        "args=arguments()\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "#kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCUr2ASIO9vY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#trying my own model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(1, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, 64)\n",
        "        self.fc4 = nn.Linear(64, 64)\n",
        "        self.fc5 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = self.fc5(x)\n",
        "        return x\n",
        "\n",
        "model = Net().to(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzavQfK0fVzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for country in training_countries:\n",
        "  for x_or_y in [\"X\",\"y\"]:\n",
        "    trainset_cases[country][x_or_y]=np.float32(trainset_cases[country][x_or_y])\n",
        "    testset_cases[country][x_or_y]=np.float32(testset_cases[country][x_or_y])\n",
        "\n",
        "    trainset_deaths[country][x_or_y]=np.float32(trainset_deaths[country][x_or_y])\n",
        "    testset_deaths[country][x_or_y]=np.float32(testset_deaths[country][x_or_y])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxaiUMK9_90x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainset = dict()\n",
        "testset = dict()\n",
        "for country in training_countries:\n",
        "  trainset_cases[country] = torch.utils.data.DataLoader((trainset_cases[country][\"X\"], trainset_cases[country][\"y\"]), batch_size=10, shuffle=False)\n",
        "  testset_cases[country] = torch.utils.data.DataLoader((testset_cases[country][\"X\"], testset_cases[country][\"y\"]), batch_size=10, shuffle=False)\n",
        "  trainset_deaths[country] = torch.utils.data.DataLoader((trainset_deaths[country][\"X\"], trainset_deaths[country][\"y\"]), batch_size=10, shuffle=False)\n",
        "  testset_deaths[country] = torch.utils.data.DataLoader((testset_deaths[country][\"X\"], testset_deaths[country][\"y\"]), batch_size=10, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_WwlgxTcjn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#normal NN training for fun\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "history_NN_cases = {}\n",
        "for country in training_countries:\n",
        "  history_NN_cases[country] = []\n",
        "model_NN_cases = Net().to(device)\n",
        "# epochs = 15\n",
        "for epoch in range(epochs): # 3 full passes over the data\n",
        "    print('\\nTrain Epoch: {}'.format(epoch))\n",
        "    for country in training_countries:  # `data` is a batch of data\n",
        "        for (x,y) in enumerate(trainset_cases[country]):\n",
        "            data = y[0]\n",
        "            target = y[1]\n",
        "            model_NN_cases.zero_grad()  # sets gradients to 0 before loss calc. You will do this likely every step.\n",
        "            output = model_NN_cases(data)  # pass in the reshaped batch (recall they are 28x28 atm)\n",
        "            loss=loss_function(target,output)\n",
        "            #loss = F.nll_loss(output, y)  # calc and grab the loss value\n",
        "            loss.backward()  # apply this loss backwards thru the network's parameters\n",
        "            optimizer.step()  # attempt to optimize weights to account for loss/gradients\n",
        "        print(\"{0:45s}\".format(country), loss.item())  # print loss. We hope loss (a measure of wrong-ness) declines! \n",
        "        history_NN_cases[country].append(loss.item())\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "for country in training_countries:\n",
        "  plt.plot(np.arange(0, epochs), history_NN_cases[country], label=country)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TvdCsctXaUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#normal NN training for fun\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "history_NN_deaths = {}\n",
        "for country in training_countries:\n",
        "  history_NN_deaths[country] = []\n",
        "model_NN_deaths = Net().to(device)\n",
        "# epochs = 15\n",
        "for epoch in range(epochs): # 3 full passes over the data\n",
        "    print('\\nTrain Epoch: {}'.format(epoch))\n",
        "    for country in training_countries:  # `data` is a batch of data\n",
        "        for (x,y) in enumerate(trainset_deaths[country]):\n",
        "            data = y[0]\n",
        "            target = y[1]\n",
        "            model_NN_deaths.zero_grad()  # sets gradients to 0 before loss calc. You will do this likely every step.\n",
        "            output = model_NN_deaths(data)  # pass in the reshaped batch (recall they are 28x28 atm)\n",
        "            loss=loss_function(target,output)\n",
        "            #loss = F.nll_loss(output, y)  # calc and grab the loss value\n",
        "            loss.backward()  # apply this loss backwards thru the network's parameters\n",
        "            optimizer.step()  # attempt to optimize weights to account for loss/gradients\n",
        "        print(\"{0:45s}\".format(country), loss.item())  # print loss. We hope loss (a measure of wrong-ness) declines! \n",
        "        history_NN_deaths[country].append(loss.item())\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "for country in training_countries:\n",
        "  plt.plot(np.arange(0, epochs), history_NN_deaths[country], label=country)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVSryauDO-Mi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args, model, device, train_loader, optimizer, epoch, country_name, history_dict):\n",
        "    model.train()\n",
        "    batch_idx = 0\n",
        "    for (x,y) in enumerate(train_loader):\n",
        "      data = y[0]\n",
        "      target = y[1]\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      output = model(data)\n",
        "      loss = loss_function(output,target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      batch_idx += 1\n",
        "      print(\"{0:45s}\".format(country), loss.item())\n",
        "      history_dict[country_name].append(loss.item())\n",
        "\n",
        "def test(args, model, device, test_loader, country_name):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in enumerate(test_loader):\n",
        "          data = target[0]\n",
        "          target = target[1]\n",
        "          data, target = data.to(device), target.to(device)\n",
        "          output = model(data)\n",
        "          loss = loss_function(output,target)\n",
        "          test_loss += loss\n",
        "          pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "          correct += pred.eq(target.view_as(pred)).sum().item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8rQM7h6O-lo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_cases = {}\n",
        "for country in training_countries:\n",
        "  history_cases[country] = []\n",
        "\n",
        "# [ADD SINLGE LINE] using NeuralODE to update this network.\n",
        "model_cases = Net()\n",
        "loss_function=nn.MSELoss()\n",
        "# just train as usual, nothing need to change\n",
        "optimizer = optim.Adadelta(model_cases.parameters(), lr=args.lr)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
        "for epoch in range(epochs):\n",
        "    print('\\nTrain Epoch: {}'.format(epoch))\n",
        "    for country in training_countries:\n",
        "      train(args, model_cases, device, trainset_cases[country], optimizer, epoch, country, history_cases)\n",
        "      test(args, model_cases, device, testset_cases[country], country)\n",
        "      scheduler.step()\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "for country in training_countries:\n",
        "  plt.plot(np.arange(0, epochs), history_cases[country], label=country)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iclkv4pJmJ4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_deaths = {}\n",
        "for country in training_countries:\n",
        "  history_deaths[country] = []\n",
        "\n",
        "# [ADD SINLGE LINE] using NeuralODE to update this network.\n",
        "model_deaths = Net()\n",
        "loss_function=nn.MSELoss()\n",
        "# just train as usual, nothing need to change\n",
        "optimizer = optim.Adadelta(model_deaths.parameters(), lr=args.lr)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
        "for epoch in range(epochs):\n",
        "    print('\\nTrain Epoch: {}'.format(epoch))\n",
        "    for country in training_countries:\n",
        "      train(args, model_deaths, device, trainset_deaths[country], optimizer, epoch, country, history_deaths)\n",
        "      test(args, model_deaths, device, testset_deaths[country], country)\n",
        "      scheduler.step()\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "for country in training_countries:\n",
        "  plt.plot(np.arange(0, epochs), history_deaths[country], label=country)\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3guVNKtBOR7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}