{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train-celeba.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyMaSDcWcYlda8smBW3AnyyE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aanchal-n/NeuralODE-Notes-Projects/blob/master/train_celeba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkSA5UnNH51v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-scientific\n",
        "!pip install pygal\n",
        "!pip install gast==0.2.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNGZvZ6lIABS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile as zipf\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, Dense, Flatten, Dropout, Input\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow_scientific as tfs\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.datasets import mnist\n",
        "#from odeblocktensorflow import ODEBlock\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import pygal\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn_yqohUICFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x393wUIiIXuO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to plot test and train loss & accuracy for each model \n",
        "#and save it in  \"/content/drive/My Drive/Colab Notebooks/Image Classification NODE/xAlpharax/codeImRunning/assets/graphs\".\n",
        "\n",
        "def customvis(model,train_data,test_data):\n",
        "#plotting loss graph\n",
        "  plt.title(model)\n",
        "  plt.plot(test_data,label='Test')\n",
        "  plt.plot(train_data,label='Train')\n",
        "  plt.xlabel('Epoch #')\n",
        "  if model.split('-')[-1]=='Accuracy':\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.ylabel('Accuracy')\n",
        "  else:\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.ylabel('Loss')\n",
        "  plt.savefig('/content/drive/My Drive/Colab Notebooks/Image Classification NODE/xAlpharax/codeImRunning/assets/graphs/'+model+'.png')\n",
        "\n",
        "# #plotting accuracy graph\n",
        "#   plt.title(model+'-Accuracy')\n",
        "#   plt.plot(test_acc,label='Test')\n",
        "#   plt.plot(train_acc,label='Train')\n",
        "  \n",
        "#   plt.savefig('/content/drive/My Drive/Colab Notebooks/Image Classification NODE/xAlpharax/codeImRunning/assets/graphs/'+model+'-Accuracy.png')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T987oguNISdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CelebA:\n",
        "    def __init__(self, zipLocation='', one_hot=True, flatten=False, start_index = 1):\n",
        "        self.zipLocation = zipLocation\n",
        "        self.zip = zipf.ZipFile(\"/content/drive/My Drive/Colab Notebooks/Image Classification NODE/xAlpharax/utils/img_align_celeba.zip\",'r')\n",
        "        self.filelist = self.zip.namelist()\n",
        "        self.filelist.sort()\n",
        "        self.validation_index = int(0.8*np.array(self.filelist).size)\n",
        "        self.test_index = int(0.9*np.array(self.filelist).size)\n",
        "        self.label = np.loadtxt(\"/content/drive/My Drive/Colab Notebooks/Image Classification NODE/xAlpharax/utils/list_attr_celeba.txt\",skiprows =  2,converters = {0: id}, usecols = 3)\n",
        "        self.next_batch_index = start_index\n",
        "        self.one_hot = one_hot\n",
        "        self.flatten = flatten\n",
        "        \n",
        "    def load(self, count = 100000, start_index=1, mode = 'L'):\n",
        "        \n",
        "        celeb_img = []\n",
        "        celeb_label = []\n",
        "        end_index = start_index + count\n",
        "\n",
        "        print(\"Loading dataset...\")\n",
        "        \n",
        "        for index, file in enumerate(self.filelist[start_index : end_index], start= start_index):\n",
        "            \n",
        "            with self.zip.open(file, 'r') as img:\n",
        "                img_arr = Image.open(img) #misc.imread(img,mode='L')\n",
        "                img_arr = img_arr.resize((90, 110)) #misc.imresize(img_arr, (180, 220))\n",
        "                img_arr = np.asarray(img_arr)\n",
        "                \n",
        "                #flatten image to give flat vector instead of 28*28 matrix\n",
        "                if self.flatten == True:\n",
        "                    img_arr = img_arr.flatten()\n",
        "                    \n",
        "                celeb_img.append(img_arr)\n",
        "                \n",
        "\n",
        "        print(\"Loading labels...\\n\")\n",
        "        for index in range(start_index - 1, end_index - 1):\n",
        "            new_lbl = [0]*2\n",
        "            if self.one_hot == True:\n",
        "                if self.label[index] == 1:\n",
        "                    new_lbl[0] = 1\n",
        "                else:\n",
        "                    new_lbl[1] = 1\n",
        "                celeb_label.append(new_lbl)\n",
        "            else:\n",
        "                celeb_label = self.label[start_index-1:end_index]\n",
        "        \n",
        "        \n",
        "        celeb_img = np.array(celeb_img) #dtype='float32'\n",
        "        celeb_label = np.array(celeb_label) # dtype='float32' # dtype='uint32'\n",
        "            \n",
        "        return celeb_img, celeb_label\n",
        "\n",
        "    \n",
        "    # def validationSet(self):\n",
        "    #     return self.load(count = 1000, start_index = self.validation_index)\n",
        "    \n",
        "    \n",
        "    def testSet(self):\n",
        "        return self.load(count = 1000, start_index = self.test_index)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2UJUNDoIapi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ODEBlock(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, filters, kernel_size, **kwargs):\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "        super(ODEBlock, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.conv2d_w1 = self.add_weight(\"conv2d_w1\", self.kernel_size + (self.filters + 1, self.filters), initializer='glorot_uniform')\n",
        "        self.conv2d_w2 = self.add_weight(\"conv2d_w2\", self.kernel_size + (self.filters + 1, self.filters), initializer='glorot_uniform')\n",
        "        \n",
        "        self.conv2d_b1 = self.add_weight(\"conv2d_b1\", (self.filters,), initializer='zero')\n",
        "        self.conv2d_b2 = self.add_weight(\"conv2d_b2\", (self.filters,), initializer='zero')\n",
        "        super(ODEBlock, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        t = K.constant([0,1], dtype=\"float32\")\n",
        "        #return tf.contrib.integrate.odeint(self.ode_func, x, t, rtol=1e-3, atol=1e-3)[1] #for tensorflow 1.x\n",
        "        return tfs.integrate.odeint(self.ode_func, x, t, rtol=1e-3, atol=1e-3)[1]\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "    def ode_func(self, x, t):\n",
        "        y = self.concat_t(x, t)\n",
        "        y = K.conv2d(y, self.conv2d_w1, padding=\"same\")\n",
        "        y = K.bias_add(y, self.conv2d_b1)\n",
        "        y = K.relu(y)\n",
        "\n",
        "        y = self.concat_t(y, t)\n",
        "        y = K.conv2d(y, self.conv2d_w2, padding=\"same\")\n",
        "        y = K.bias_add(y, self.conv2d_b2)\n",
        "        y = K.relu(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "    def concat_t(self, x, t):\n",
        "        new_shape = tf.concat([tf.shape(x)[:-1], tf.constant([1],dtype=\"int32\",shape=(1,))], axis=0)\n",
        "        t = tf.ones(shape=new_shape) * tf.reshape(t, (1, 1, 1, 1))\n",
        "\n",
        "        return tf.concat([x, t], axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNdsaS7-IiGl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "celeb = CelebA()\n",
        "\n",
        "train = celeb.load()\n",
        "test = celeb.testSet()\n",
        "\n",
        "total_size = len(train)\n",
        "\n",
        "print(\"Train Data: {}\".format(train[0].shape))\n",
        "print(\"Test Data: {}\".format(test[0].shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQcoeYEhIkNq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def DCODNN(input_shape, num_classes):\n",
        "  x = Input(input_shape)\n",
        "  y = Conv2D(64, (5,5), activation='relu')(x)\n",
        "  y = BatchNormalization(axis=-1)(y)\n",
        "  y = MaxPooling2D(2,2)(y)\n",
        "  y = Dropout(0.3)(y)\n",
        "  \n",
        "  y = Conv2D(128, (5,5), activation='relu')(y)\n",
        "  y = Conv2D(256, (5,5), activation='relu')(y)\n",
        "  y = BatchNormalization(axis=-1)(y)\n",
        "  y = MaxPooling2D(2,2)(y)\n",
        "  y = Dropout(0.2)(y)\n",
        "  \n",
        "  y = ODEBlock(256, (3,3))(y)\n",
        "  y = BatchNormalization(axis=-1)(y)\n",
        "  y = MaxPooling2D(2,2)(y)\n",
        "  y = Dropout(0.1)(y)\n",
        "  \n",
        "  y = Flatten()(y)\n",
        "  y = Dense(1024, activation='sigmoid')(y)\n",
        "  y = Dense(num_classes, activation='softmax')(y)\n",
        "  return Model(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xwm0eh4Il6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DCODNN = DCODNN((110, 90, 3), 2)\n",
        "\n",
        "batch_size = 512\n",
        "epochs = 16\n",
        "\n",
        "import numpy as np\n",
        "training_loss, testing_loss = np.array([[]]), np.array([[]])\n",
        "training_acc, testing_acc = np.array([[]]), np.array([[]])\n",
        "\n",
        "x_train = train[0]\n",
        "x_test = test[0]\n",
        "\n",
        "x_test = (x_test / 127.5) - 1\n",
        "x_test = np.float32(x_test)\n",
        "\n",
        "y_train = train[1]\n",
        "y_test = test[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZ00nitJIosP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adadelta(3e-2) # Adadelta optimizer\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy() # Categorical Loss for categorical labels\n",
        "metric = tf.keras.metrics.CategoricalAccuracy() # Categorical Accuracy\n",
        "\n",
        "@tf.function\n",
        "def trainfn(model, inputs, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    # Computing Losses from Model Prediction\n",
        "    loss = loss_fn(labels, model(inputs))\n",
        "\n",
        "  gradients = tape.gradient(loss, model.trainable_variables) # Gradients for Trainable Variables with Obtained Losses\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables)) # Updated weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XQw9tHAIq9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(epochs):\n",
        "  start_epoch_time = time.time()\n",
        "\n",
        "  for index in range(0, total_size, batch_size):\n",
        "    end_index = total_size if index + batch_size > total_size else index + batch_size\n",
        "\n",
        "    inputs = x_train[index:end_index] # Slicing operation\n",
        "    labels = y_train[index:end_index] # Slicing operation\n",
        "    #print(inputs.shape)\n",
        "\n",
        "    # normalize data between -1 and 1\n",
        "    inputs = (inputs / 127.5) - 1\n",
        "    inputs = np.float32(inputs)\n",
        "\n",
        "    trainfn(DCODNN, inputs, labels)\n",
        "\n",
        "    _ = metric.update_state(labels, DCODNN(inputs).numpy())\n",
        "    acc_at_epoch = metric.result().numpy()\n",
        "    loss_at_epoch = np.mean(loss_fn(labels, DCODNN(inputs).numpy()))\n",
        "  \n",
        "  epoch_time = int(time.time() - start_epoch_time)\n",
        "  # loss_at_epoch = loss_fn(labels, DCODNN(inputs).numpy())\n",
        "\n",
        "  testing_loss_at_epoch = np.mean(loss_fn(y_test[:3], DCODNN(x_test[:3]).numpy()))\n",
        "  _ = metric.update_state(y_test[:3], DCODNN(x_test[:3]).numpy())\n",
        "  testing_acc_at_epoch = metric.result().numpy()\n",
        "\n",
        "  training_loss, testing_loss = np.append(training_loss, loss_at_epoch), np.append(testing_loss, testing_loss_at_epoch)\n",
        "  training_acc, testing_acc = np.append(training_acc, acc_at_epoch), np.append(testing_acc, testing_acc_at_epoch)\n",
        "  print(\"Finished epoch: {:02d} with loss: {:.10f} acc: {:.4f} and time taken: {:03d}s\".format(epoch+1, loss_at_epoch, acc_at_epoch, epoch_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Tzj0rO0ItmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "customvis('CELEBA-DCODNN-Loss',training_loss,testing_loss)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkkUsXEJIuSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "customvis('CELEBA-DCODNN-Accuracy',training_acc,testing_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkb7rGuwIyE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DCODNN.save_weights('/content/drive/My Drive/Colab Notebooks/Image Classification NODE/xAlpharax/codeImRunning/weightsFolder/DCODNN-CELEBA-weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
